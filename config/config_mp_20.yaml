random_seed: 42
batch_size: 20
train_ratio: 0.6
val_ratio: 0.2
test_ratio: 0.2
workers: 0
epochs: 2000
adjust_every: 12
validate_every_epoch: 50


# model config
class_dim: 100
pos_dim: 3
lattice_dim: 6
hidden_dim: 64
latent_dim: 64

k: 20
encoder1_dim: [64,64]
num_encoder2_layers: 2
decoder1_dims: [64, 64]
decoder2_dims: [64, 64]
decoder_lattice_dims: [64,64]
decoder_lattice_dims2: [64,64]
codebooksize1: 128
codebooksize2: 128
codebooksizel: 256
num_quantizers1: 4
num_quantizers2: 8
num_quantizersl: 8
sample_codebook_temp: 0.1
lambdas: [15, 100, 0.25, 200, 0.1]

# optimizer config
warmup_epochs: 10
warmup_factor: 0.1
grad_clip: 1.0
optimizer:
  type: Adam
  params:
    lr: 0.00005
    weight_decay: 0.0001

# scheduler config
scheduler:
  type: CosineAnnealingLR
  params:
    T_max: 500
    eta_min: 0.0000000001
# scheduler:
#   type: StepLR
#   params:
#     step_size: 50  
#     gamma: 0.5 